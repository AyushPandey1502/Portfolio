# Now that you understand the basic idea of recognizing gestures, let's start a practical project to learn more about it.
# In this project, we'll focus on counting the number of open fingers, measuring frames per second (fps), and other interesting stuffs.



# I hope you guys have already installed the required dependencies told by Simran earlier.
# If you not install till now, pls install the libraries right now

# So the command for installing the mediapipe module is pip install mediapipe and for openCV is pip install opencv-python

# Before moving on we have to first import the libraries required for our project

# Importing Libraries
import cv2 # This is a popular library for computer vision tasks like image processing and video processing
import mediapipe as mp # This library provides pre-built machine learning models for gesture tracking and detection
import time  # Time is a inbuilt library of python which we are going use for fps calculation

# In order to capture the video from the webcam we are going to use the VideoCapture function provided by opencv module
# This line creates a video capture object (video) that creates a connection between your program and your webcam
# We have given argument as 0 as we want to access our device webcam otherwise set it to 1 if you want to capture video from an external webcam
video = cv2.VideoCapture(0)

# Here we have written a conditional code that checks whether your camera is open of not
# It will return an error message and exit the program if the program is not able to capture the video from webcam
if not video.isOpened():
    print("Error: Unable to open video source.")
    exit()

# This line creates a hand tracking object (mpHands) from the MediaPipe library. Also setting max hands as 1.

#Actually mediapipe contain many prebuilt models for gesture detection
mpHands = mp.solutions.hands # Here we are actually importing the hand tracking model from mediapipe

# In the second line we inititalize and create an instance of the hand tracking model
hands = mpHands.Hands(max_num_hands=1) # We have set our max_num_hands to 1 as we want our program to detect only one hand at a time.

# Here we are creating a drawing utility object (mpDraw) used to draw the hand landmarks on the video frame.
mpDraw = mp.solutions.drawing_utils

# Here we create a list that stores the landmark IDs (indices) corresponding to the fingertips of each finger (thumb to little finger)
tipIds = [4, 8, 12, 16, 20]

# This list stores the names of each finger corresponding to their tip IDs, which we later use for showing the name of the finger when a single finger is open
fingerName = ['thumb', 'index finger', 'middle finger', 'ring finger', 'little finger']

#We actually inititalize prevTime variable to 0 inorder to avoid the error when our program is started as for fps calculation there is no frame before the first frame
prevTime = 0

# We know that video is actually a collection of a frames in a sequence.
# So in order to create track of each frame continuously we are going to create an infinite loop using while loop.

# This loop continuously captures frames from the webcam and processes them.
while True:
    # This line reads a frame from the webcam. success is a boolean indicating if the frame was read successfully. img stores the captured frame as an image.
    success, img = video.read()  # Now in order to read frame we are using the read function provided by opencv that returns the image and a boolean value that
    # your frame was read successfully.

    # Actually the image return by read function is inverted so we are flipping the image horizontally so that the image will be mirrored.
    img = cv2.flip(img, 1)

    # if not success:
    #     print("Error: Unable to read frame from video source.")
    #     break

    # Actually opencv return image in BGR format but our mediapipe process image in RGB format so we have to change the image format
    # Inorder to do so we are using color convert function of opencv module.
    # This line converts the image from BGR (OpenCV's default color format) to RGB format, which is expected by the MediaPipe hand tracking model. This step might be unnecessary depending on the MediaPipe version.
    imgRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

    #  So now to start processing the image for landmarks detection we are using the process function provided by mediapipe
    #  This line processes the current frame (imgRGB) using the hand tracking model (hands). The results variable will contain information about the detected hands, if any.
    results = hands.process(imgRGB)

    lmList = [] # Here we are inititalizing an empty list that will be used to keep track of the detected hand


    # This checks if the results from the hand tracking model (mpHands.process(imgRGB)) contain any information about detected hands (multi_hand_landmarks). If hands are detected (not None), the inner loop executes.
    if results.multi_hand_landmarks:
        # This loop iterates through each detected hand (handLms) in the results. Each handLms object contains information about the landmarks for that specific hand
        for handLms in results.multi_hand_landmarks:
            for id, lm in enumerate(handLms.landmark):
                # This line gets the height (h), width (w), and number of channels (c) of the image (img). This information is needed to convert the normalized landmark coordinates (lm.x and lm.y) from the hand tracking model (between 0.0 and 1.0) to actual pixel coordinates within the image.
                h, w, c = img.shape
                # This calculates the pixel coordinates (cx, cy) for the current landmark. It multiplies the normalized x-coordinate (lm.x) by the image width (w) and the normalized y-coordinate (lm.y) by the image height (h), and then converts the results to integers using int().
                cx, cy = int(lm.x * w), int(lm.y * h)
                lmList.append([id, cx, cy])

                # Here we are using a conditional if statement that helps in pointing out a specific index which is here, the thumb
                if id == 4:
                    cv2.circle(img, (cx, cy), 15, (255, 0, 255), cv2.FILLED)
                    # So circle fucntion will draw a filled circle around your thumb's tip

            # This line will draw the landmarks and their connection between them.
            mpDraw.draw_landmarks(img, handLms, mpHands.HAND_CONNECTIONS)
    # print(lmList)

    # If the hand is detected , then lmList will contain the pixel coordinates of each landmark with their IDs.
    if len(lmList):
        fingers = [] # Here we created an empty list that will be used to store the status of finger whether it is open or not
        # If at a particular index 0 is stored that means the finger corresponding to same index in fingerName list is closed
        # otherwise it means that your particular finger is open

        # As Simran already told you how to check whether your finger is open or not
        # Can any one of you tell me the logic.

        # thumb

        # So if your thumb's tip landmark is to the right of your knuckle base landmark then we consider open thumb as close otherwise we will
        # consider as open
        if ((lmList[tipIds[0]][1] > lmList[tipIds[0] - 1][1] and lmList[tipIds[0]][2] <= lmList[tipIds[0] - 1][2]) or (
                lmList[tipIds[0]][1] < lmList[tipIds[0] - 1][1] and lmList[tipIds[0]][2] >= lmList[tipIds[0] - 1][2])):
            fingers.append(1)
        else:
            fingers.append(0)

        # fingers

        # If the tip landmark id of each finger is above their base knuckle landmark we will consider our finger as open and append 1 in list otherwise we will append 0
        for id in range(1, 5):
            if lmList[tipIds[id]][2] < lmList[tipIds[id] - 2][2]:
                fingers.append(1)
            else:
                fingers.append(0)

        totalFingers = sum(fingers) # Here we are finding the number of fingers open as we that we have appended 1 for finger which is open
        # So your totalFingers variable will store number of fingers open
        cv2.putText(img, str(totalFingers), (575, 45), cv2.FONT_HERSHEY_PLAIN, 3, (0, 255, 0), 3)

        # So one of our objective is completed that to find finger counts.

        # So to show the name of the finger when a single finger is open we first have to check that the number of fingers open in totalFingers varible
        # If it is 1 then it starts to check which finger is open by iteration over your fingers list which is storing the stautus of all the fingers
        # whether it is open or not
        if totalFingers == 1:
            for fin in range(5):
                if fingers[fin] == 1: # So when it detect 1 at a particular index then it will store the name of the finger in the upFinger variable
                    # corresponding to that index from fingerName list which we have created earlier
                    upFinger = fingerName[fin]
                    # Now we have detected the finger, we have to write the finger name on the frame, that is one of our objective
                    # If you want to write text on the frame, we can use putText function of opencv that takes frame as first argument
                    cv2.putText(img, str(upFinger), (275, 445), cv2.FONT_HERSHEY_PLAIN, 3, (0, 255, 0), 3)

        elif totalFingers == 5:
            cv2.putText(img, 'palm', (275, 445), cv2.FONT_HERSHEY_PLAIN, 3, (0, 255, 0), 3)

        elif totalFingers == 0:
            cv2.putText(img, 'fist', (275, 445), cv2.FONT_HERSHEY_PLAIN, 3, (0, 255, 0), 3)

        totalFingers = sum(fingers)
        print(totalFingers)



    #  Actually fps is the number of frames it detect per second
    #  so inorder to calculate the fps we are going to find the time difference between your last frame and present frame
    currTime = time.time() # So we are using time function to store the time of current frame from your device
    fps = 1 / (currTime - prevTime) # So fps is just the reciprocal of the time difference between your present frame and last frame
    prevTime = currTime # Here we are updating the prevTime var with the currTime var

    cv2.putText(img, str(int(fps)), (10, 70), cv2.FONT_HERSHEY_SIMPLEX, 3, (255, 0, 255), 3)
    # This displays the processed image (with hand landmarks, finger count, and FPS) in a window titled "Image".



    cv2.imshow("Image", img) #We use imshow function to display the processed image
    if cv2.waitKey(0) & 0xFF == ord('q'): # To hold the window and ensure the frame of video is displayed for at least 1ms.
        # if 'q' is pressed then it breaks the infinite loop
        break

# It is a good practice to release camera and destroy windows when our program is complete.
# Releasing windows helps in freeing up system resources and preventing memory leaks.
video.release()  # This releases the webcam resource.
cv2.destroyAllWindows()  #  This closes all OpenCV windows.

